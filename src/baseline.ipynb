{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "from configurations import *\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(f'{LOAD_DATA_PATH}/{DATA_CSV_NAME}')\n",
    "df = df.sort_values(by=['d', 't'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simulate_missing_predictions(df: DataFrame):\n",
    "    predicted_rows = []\n",
    "    user_groups = df.groupby('uid')\n",
    "    \n",
    "    for user, user_data in user_groups:\n",
    "        coords = np.array(list(zip(user_data['x'], user_data['y'])))\n",
    "        d_values = user_data['d'].values \n",
    "        t_values = user_data['t'].values\n",
    "        \n",
    "        x_preds = (coords[:-2, 0] + coords[2:, 0]) / 2\n",
    "        y_preds = (coords[:-2, 1] + coords[2:, 1]) / 2\n",
    "\n",
    "        coords_len = len(coords) - 1\n",
    "\n",
    "        predicted_rows.append({\n",
    "                'uid': user,\n",
    "                'd': d_values[0],\n",
    "                't': t_values[0],\n",
    "                'x': coords[0, 0],\n",
    "                'y': coords[0, 1]\n",
    "        })\n",
    "\n",
    "        predicted_rows.extend([{\n",
    "                'uid': user,\n",
    "                'd': d_values[i],\n",
    "                't': t_values[i],\n",
    "                'x': round(x_preds[i - 1]),\n",
    "                'y': round(y_preds[i - 1])\n",
    "            } for i in range(1, coords_len)])\n",
    "\n",
    "        predicted_rows.append({\n",
    "                'uid': user,\n",
    "                'd': d_values[coords_len],\n",
    "                't': t_values[coords_len],\n",
    "                'x': coords[coords_len, 0],\n",
    "                'y': coords[coords_len, 1]\n",
    "        })\n",
    "    \n",
    "    predicted_df = pd.DataFrame(predicted_rows)\n",
    "    return predicted_df\n",
    "\n",
    "df_predictions = simulate_missing_predictions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 Yahoo Japan Corporation\n",
    "#  \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a \n",
    "# copy of this software and associated documentation files (the \"Software\"), \n",
    "# to deal in the Software without restriction, including without limitation \n",
    "# the rights to use, copy, modify, merge, publish, distribute, sublicense, \n",
    "# and/or sell copies of the Software, and to permit persons to whom the \n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "# \n",
    "# The above copyright notice and this permission notice shall be included \n",
    "# in all copies or substantial portions of the Software.\n",
    "# \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS \n",
    "# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL \n",
    "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER \n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING \n",
    "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER \n",
    "# DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def gen_ngram_list(seq, n):\n",
    "    ngram_list = list()\n",
    "    if len(seq) < n:\n",
    "        return ngram_list\n",
    "\n",
    "    for i in range(len(seq) - n + 1):\n",
    "        ngram = seq[i: i + n]\n",
    "        ngram_list.append(ngram)\n",
    "\n",
    "    return ngram_list\n",
    "        \n",
    "def calc_distance(point1, point2, scale_factor=1.):\n",
    "    x_diff = point2[0] - point1[0]\n",
    "    y_diff = point2[1] - point1[1]\n",
    "    return np.sqrt(x_diff ** 2. + y_diff ** 2.) / scale_factor\n",
    "\n",
    "def calc_point_proximity(point1, point2, beta):\n",
    "    distance = calc_distance(point1, point2)\n",
    "    return np.exp(-beta * distance)\n",
    "\n",
    "def calc_ngram_proximity(ngram1, ngram2, beta):\n",
    "    point_proximity_list = list()\n",
    "    for point1, point2 in zip(ngram1, ngram2):\n",
    "        point_proximity_list.append(calc_point_proximity(point1, point2, beta))\n",
    "        \n",
    "    return np.prod(point_proximity_list)\n",
    "\n",
    "def calc_geo_p_n(sys_seq, ans_seq, n, beta):\n",
    "    sys_ngram_list = gen_ngram_list(sys_seq, n)\n",
    "    ans_ngram_list = gen_ngram_list(ans_seq, n)\n",
    "\n",
    "    edge_list = list()\n",
    "    for sys_id, sys_ngram in enumerate(sys_ngram_list):\n",
    "        for ans_id, ans_ngram in enumerate(ans_ngram_list):\n",
    "            ngram_pair = (sys_id, ans_id)\n",
    "            proximity = calc_ngram_proximity(sys_ngram, ans_ngram, beta)\n",
    "            edge_list.append((ngram_pair, proximity))\n",
    "   \n",
    "    edge_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    proximity_sum = 0.\n",
    "    proximity_cnt = 0\n",
    "\n",
    "    while len(edge_list) > 0:\n",
    "        best_edge = edge_list[0]\n",
    "        best_edge_ngram_pair = best_edge[0]\n",
    "        proximity = best_edge[1]\n",
    "        \n",
    "        proximity_sum += proximity\n",
    "        proximity_cnt += 1\n",
    "        best_edge_sys_id, best_edge_ans_id = best_edge_ngram_pair\n",
    "        \n",
    "        new_edge_list = list()\n",
    "        for edge in edge_list:\n",
    "            ngram_pair = edge[0]\n",
    "            sys_id, ans_id = ngram_pair\n",
    "            if sys_id == best_edge_sys_id:\n",
    "                continue\n",
    "            if ans_id == best_edge_ans_id:\n",
    "                continue\n",
    "                \n",
    "            new_edge_list.append(edge)\n",
    "            \n",
    "        edge_list = new_edge_list\n",
    "        \n",
    "    geo_p_n = proximity_sum / float(proximity_cnt)\n",
    "    \n",
    "    return geo_p_n\n",
    "\n",
    "def check_arguments(sys_seq, ans_seq):\n",
    "    # check the input arguments\n",
    "\n",
    "    # the trajectory length\n",
    "    if len(sys_seq) == 0:\n",
    "        raise ValueError(\"The length of the generated trajectory is 0.\")\n",
    "    if len(ans_seq) == 0:\n",
    "        raise ValueError(\"The length of the reference trajectory is 0.\")\n",
    "\n",
    "    if len(sys_seq) != len(ans_seq):\n",
    "        raise ValueError(\n",
    "            \"The length doesn't match between the generated and reference trajectories.\")\n",
    "\n",
    "    # the number of columns\n",
    "    sys_len_counter = Counter()\n",
    "    ans_len_counter = Counter()\n",
    "    for sys_step, ans_step in zip(sys_seq, ans_seq):\n",
    "        sys_len_counter[len(sys_step)] += 1\n",
    "        ans_len_counter[len(ans_step)] += 1\n",
    "    sys_len_list = list(sys_len_counter.keys())\n",
    "    ans_len_list = list(ans_len_counter.keys())\n",
    "    if len(sys_len_list) != 1:\n",
    "        raise ValueError(\"The numbers of columns of the generated trajectory are inconsistent: {}\".format(sys_len_counter))\n",
    "    if len(ans_len_list) != 1:\n",
    "        raise ValueError(\"The numbers of columns of the reference trajectory are inconsistent: {}\".format(ans_len_counter))\n",
    "\n",
    "    # only (d, t, x, y) and (uid, d, t, x, y) are acceptable, and the format must be\n",
    "    # the same between the generated and reference\n",
    "    sys_columns = sys_len_list[0]\n",
    "    ans_columns = ans_len_list[0]\n",
    "    if sys_columns != ans_columns:\n",
    "        raise ValueError(\"The numbers of columns are different between the generated and reference trajectories.\")\n",
    "    if sys_columns not in {4, 5}:\n",
    "        raise ValueError(\"The numbers of columns must be 4, (d, t, x, y), or 5, (uid, d, t, x, y).\")\n",
    "\n",
    "    # if the format is (uid, d, t, x, y), drop the uid column, making it (d, t, x, y)\n",
    "    if sys_columns == 5:\n",
    "        sys_seq = [step[1:] for step in sys_seq]\n",
    "        ans_seq = [step[1:] for step in ans_seq]\n",
    "\n",
    "    # consistency of day and time\n",
    "    for idx, (sys_step, ans_step) in enumerate(zip(sys_seq, ans_seq)):\n",
    "        sys_d, sys_t = sys_step[:2]\n",
    "        ans_d, ans_t = ans_step[:2]\n",
    "        if not (sys_d == ans_d and sys_t == ans_t):\n",
    "            raise ValueError(\n",
    "                \"Day and time are not the same at step {}, \"\n",
    "                \"d={} and t={} for generated while d={} and t={} for reference.\".format(\n",
    "                    idx, sys_d, sys_t, ans_d, ans_t))\n",
    "\n",
    "    # sort by day and time just in case\n",
    "    sys_seq.sort(key=lambda x: (x[0], x[1]))\n",
    "    ans_seq.sort(key=lambda x: (x[0], x[1]))\n",
    "    return sys_seq, ans_seq\n",
    "\n",
    "def split_trajectory_by_day(seq):\n",
    "    dict_by_day = dict()\n",
    "    for d, t, x, y in seq:\n",
    "        if d not in dict_by_day.keys():\n",
    "            dict_by_day[d] = list()\n",
    "        dict_by_day[d].append((x, y))\n",
    "\n",
    "    return dict_by_day\n",
    "\n",
    "def calc_geobleu_orig_wrapper_humob23(arg):\n",
    "    sys_seq = arg[0]\n",
    "    ans_seq = arg[1]\n",
    "    return calc_geobleu_orig(sys_seq, ans_seq, max_n=3, beta=0.5, weights=None)\n",
    "\n",
    "def calc_dtw_orig_wrapper_humob23(arg):\n",
    "    sys_seq = arg[0]\n",
    "    ans_seq = arg[1]\n",
    "    return calc_dtw_orig(sys_seq, ans_seq, scale_factor=2.)\n",
    "\n",
    "# == public method ==\n",
    "def calc_geobleu_orig(sys_seq, ans_seq, max_n=3, beta=0.5, weights=None):\n",
    "    p_n_list = list()\n",
    "    seq_len_min = min(len(sys_seq), len(ans_seq))\n",
    "    max_n_alt = min(max_n, seq_len_min)\n",
    "\n",
    "    for i in range(1, max_n_alt + 1):\n",
    "        p_n = calc_geo_p_n(sys_seq, ans_seq, i, beta)\n",
    "        p_n_list.append(p_n)\n",
    "        \n",
    "    brevity_penalty = 1. if len(sys_seq) > len(ans_seq) \\\n",
    "        else np.exp(1. - len(ans_seq) / float(len(sys_seq)))\n",
    "        \n",
    "    return brevity_penalty * stats.mstats.gmean(p_n_list)\n",
    "\n",
    "def calc_dtw_orig(sys_seq, ans_seq, scale_factor=1.):\n",
    "    n, m = len(sys_seq), len(ans_seq)\n",
    "    dtw_matrix = np.zeros((n + 1, m + 1))\n",
    "\n",
    "    for i in range(n + 1):\n",
    "        for j in range(m + 1):\n",
    "            dtw_matrix[i, j] = np.inf\n",
    "    dtw_matrix[0] = 0\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            cost = calc_distance(sys_seq[i - 1], ans_seq[j - 1], scale_factor=scale_factor)\n",
    "            last_min = np.min([dtw_matrix[i - 1, j], dtw_matrix[i, j - 1], dtw_matrix[i - 1, j - 1]])\n",
    "            dtw_matrix[i, j] = cost + last_min\n",
    "    return dtw_matrix[-1, -1]\n",
    "\n",
    "def calc_geobleu(sys_seq, ans_seq, processes=4):\n",
    "    # check the input arguments\n",
    "    sys_seq, ans_seq = check_arguments(sys_seq, ans_seq)\n",
    "\n",
    "    # split the trajectories by day\n",
    "    sys_dict_by_day = split_trajectory_by_day(sys_seq)\n",
    "    ans_dict_by_day = split_trajectory_by_day(ans_seq)\n",
    "\n",
    "    # loop over days, calculating geobleu for each day\n",
    "    arg_list = list()\n",
    "    for d in ans_dict_by_day.keys():\n",
    "        arg = (\n",
    "            sys_dict_by_day[d],\n",
    "            ans_dict_by_day[d],\n",
    "        )\n",
    "        arg_list.append(arg)\n",
    "\n",
    "    with Pool(processes=processes) as p:\n",
    "        geobleu_val_list = p.map(calc_geobleu_orig_wrapper_humob23, arg_list)\n",
    "\n",
    "    # return the average value over days\n",
    "    return np.mean(geobleu_val_list)\n",
    "\n",
    "def calc_dtw(sys_seq, ans_seq, processes=4):\n",
    "    # check the input arguments\n",
    "    sys_seq, ans_seq = check_arguments(sys_seq, ans_seq)\n",
    "\n",
    "    # split the trajectories by day\n",
    "    sys_dict_by_day = split_trajectory_by_day(sys_seq)\n",
    "    ans_dict_by_day = split_trajectory_by_day(ans_seq)\n",
    "\n",
    "    # loop over days, calculating dtw for each day\n",
    "    arg_list = list()\n",
    "    for d in ans_dict_by_day.keys():\n",
    "        arg = (\n",
    "            sys_dict_by_day[d],\n",
    "            ans_dict_by_day[d],\n",
    "        )\n",
    "        arg_list.append(arg)\n",
    "\n",
    "    with Pool(processes=processes) as p:\n",
    "        dtw_val_list = p.map(calc_dtw_orig_wrapper_humob23, arg_list)\n",
    "\n",
    "    # the average value over days\n",
    "    return np.mean(dtw_val_list)\n",
    "\n",
    "def calc_geobleu_single(sys_seq, ans_seq):\n",
    "    # check the input arguments\n",
    "    sys_seq, ans_seq = check_arguments(sys_seq, ans_seq)\n",
    "\n",
    "    # split the trajectories by day\n",
    "    sys_dict_by_day = split_trajectory_by_day(sys_seq)\n",
    "    ans_dict_by_day = split_trajectory_by_day(ans_seq)\n",
    "\n",
    "    # loop over days, calculating geobleu for each day\n",
    "    geobleu_val_list = list()\n",
    "    for d in ans_dict_by_day.keys():\n",
    "        geobleu_val = calc_geobleu_orig(\n",
    "            sys_dict_by_day[d],\n",
    "            ans_dict_by_day[d],\n",
    "            max_n=3,\n",
    "            beta=0.5,\n",
    "            weights=None)\n",
    "        geobleu_val_list.append(geobleu_val)\n",
    "\n",
    "    # return the average value over days\n",
    "    return np.mean(geobleu_val_list)\n",
    "\n",
    "def calc_dtw_single(sys_seq, ans_seq):\n",
    "    # check the input arguments\n",
    "    sys_seq, ans_seq = check_arguments(sys_seq, ans_seq)\n",
    "\n",
    "    # split the trajectories by day\n",
    "    sys_dict_by_day = split_trajectory_by_day(sys_seq)\n",
    "    ans_dict_by_day = split_trajectory_by_day(ans_seq)\n",
    "\n",
    "    # loop over days, calculating dtw for each day\n",
    "    dtw_val_list = list()\n",
    "    for d in ans_dict_by_day.keys():\n",
    "        dtw_val = calc_dtw_orig(\n",
    "            sys_dict_by_day[d],\n",
    "            ans_dict_by_day[d],\n",
    "            scale_factor=2.)\n",
    "        dtw_val_list.append(dtw_val)\n",
    "\n",
    "    # the average value over days\n",
    "    return np.mean(dtw_val_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geobleu\n",
    "import numpy as np\n",
    "\n",
    "for user in df['uid'].unique():\n",
    "    # Datos reales y predichos para el usuario\n",
    "    ref = list(df[df['uid'] == user].itertuples(index=False, name=None))\n",
    "    pred = list(df[df['uid'] == user].itertuples(index=False, name=None))\n",
    "    # pred = list(df_predictions[df_predictions['uid'] == user].to_records(index=False))\n",
    "    # # pred = pred.drop(columns=['uid'])\n",
    "\n",
    "    score = calc_geobleu(ref, ref, processes=3)\n",
    "    \n",
    "    # # GEO-BLEU día por día\n",
    "    # geobleu_scores = np.array([])\n",
    "    # for day in ref['d'].unique():\n",
    "    #     ref_day = list(ref[ref['d'] == day].to_records(index=False))\n",
    "    #     pred_day = list(pred[pred['d'] == day].to_records(index=False))\n",
    "\n",
    "    #     score = geobleu.calc_geobleu(pred_day, ref_day, processes=3)\n",
    "    #     geobleu_scores.append(score)\n",
    "    #     # print(f\"User {user}, Day {day}: GEO-BLEU score: {score}\")\n",
    "\n",
    "    # print(geobleu_scores)\n",
    "    \n",
    "    # DTW para toda la trayectoria\n",
    "    # distance, _ = fastdtw(ref, pred, dist=euclidean)\n",
    "    # print(f\"User {user}: DTW distance: {distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean distance:\", mean_distance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
